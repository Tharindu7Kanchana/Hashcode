{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tharindu7Kanchana/Hashcode/blob/master/NN_Scratch_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvO8E0SomcTy"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import randrange\n",
        "from csv import reader\n",
        "from math import exp"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwXEttDLmOib",
        "outputId": "9ee5551b-aaa7-4594-ee1d-675bb91bba4f"
      },
      "source": [
        "!git clone 'https://github.com/sachinkavindaa/Machine-learning-with-Python'"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Machine-learning-with-Python' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR1oCFDjjc9R"
      },
      "source": [
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "    dataset = list()\n",
        "    with open(filename, 'r') as file:\n",
        "      csv_reader = reader(file)\n",
        "      csv_reader.__next__()\n",
        "      for row in csv_reader:\n",
        "          if not row:\n",
        "            continue\n",
        "          dataset.append(row)\n",
        "    return dataset"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIXcrd52j-xk"
      },
      "source": [
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = float(row[column].strip())"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9aYfhcml4W4"
      },
      "source": [
        "# Convert string column to integer\n",
        "def str_column_to_int(dataset, column):\n",
        "\tclass_values = [row[column] for row in dataset]\n",
        "\tunique = set(class_values)\n",
        "\tlookup = dict()\n",
        "\tfor i, value in enumerate(unique):\n",
        "\t\tlookup[value] = i\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = lookup[row[column]]\n",
        "\treturn lookup"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "819lQ8Onl8V3"
      },
      "source": [
        "# Find the min and max values for each column\n",
        "def dataset_minmax(dataset):\n",
        "\tminmax = list()\n",
        "\tstats = [[min(column), max(column)] for column in zip(*dataset)]\n",
        "\treturn stats\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrmdvl0rmF8N"
      },
      "source": [
        "# Rescale dataset columns to the range 0-1\n",
        "def normalize_dataset(dataset, minmax):\n",
        "\tfor row in dataset:\n",
        "\t\tfor i in range(len(row)-1):\n",
        "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M37dpjIymHFN"
      },
      "source": [
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / n_folds)\n",
        "\tfor i in range(n_folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaHcvoK1mHCH"
      },
      "source": [
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TcQTBu_mG_W"
      },
      "source": [
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\tscores = list()\n",
        "\tfor fold in folds:\n",
        "\t\ttrain_set = list(folds)\n",
        "\t\ttrain_set.remove(fold)\n",
        "\t\ttrain_set = sum(train_set, [])\n",
        "\t\ttest_set = list()\n",
        "\t\tfor row in fold:\n",
        "\t\t\trow_copy = list(row)\n",
        "\t\t\ttest_set.append(row_copy)\n",
        "\t\t\trow_copy[-1] = None\n",
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
        "\t\tactual = [row[-1] for row in fold]\n",
        "\t\taccuracy = accuracy_metric(actual, predicted)\n",
        "\t\tscores.append(accuracy)\n",
        "\treturn scores"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvkix2-NmG7a"
      },
      "source": [
        "# Calculate neuron activation for an input\n",
        "def activate(weights, inputs):\n",
        "\tactivation = weights[-1]\n",
        "\tfor i in range(len(weights)-1):\n",
        "\t\tactivation += weights[i] * inputs[i]\n",
        "\treturn activation"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqTgRsesmYMH"
      },
      "source": [
        "# Transfer neuron activation\n",
        "def transfer(activation):\n",
        "\treturn 1.0 / (1.0 + exp(-activation))"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeUaJbRGmbBV"
      },
      "source": [
        "# Forward propagate input to a network output\n",
        "def forward_propagate(network, row):\n",
        "\tinputs = row\n",
        "\tfor layer in network:\n",
        "\t\tnew_inputs = []\n",
        "\t\tfor neuron in layer:\n",
        "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
        "\t\t\tneuron['output'] = transfer(activation)\n",
        "\t\t\tnew_inputs.append(neuron['output'])\n",
        "\t\tinputs = new_inputs\n",
        "\treturn inputs"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv-V8QhImeT-"
      },
      "source": [
        "# Calculate the derivative of an neuron output\n",
        "def transfer_derivative(output):\n",
        "\treturn output * (1.0 - output)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy1wIfnNmik5"
      },
      "source": [
        "# Backpropagate error and store in neurons\n",
        "def backward_propagate_error(network, expected):\n",
        "\tfor i in reversed(range(len(network))):\n",
        "\t\tlayer = network[i]\n",
        "\t\terrors = list()\n",
        "\t\tif i != len(network)-1:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\terror = 0.0\n",
        "\t\t\t\tfor neuron in network[i + 1]:\n",
        "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
        "\t\t\t\terrors.append(error)\n",
        "\t\telse:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\tneuron = layer[j]\n",
        "\t\t\t\terrors.append(expected[j] - neuron['output'])\n",
        "\t\tfor j in range(len(layer)):\n",
        "\t\t\tneuron = layer[j]\n",
        "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enXiil3FmnAa"
      },
      "source": [
        "# Update network weights with error\n",
        "def update_weights(network, row, l_rate):\n",
        "\tfor i in range(len(network)):\n",
        "\t\tinputs = row[:-1]\n",
        "\t\tif i != 0:\n",
        "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
        "\t\tfor neuron in network[i]:\n",
        "\t\t\tfor j in range(len(inputs)):\n",
        "\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
        "\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h84wD-Oomm8d"
      },
      "source": [
        "# Train a network for a fixed number of epochs\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "    for epoch in range(n_epoch):\n",
        "      sum_error = 0\n",
        "      for row in train:\n",
        "        outputs = forward_propagate(network, row)\n",
        "        expected = [0 for i in range(n_outputs)]\n",
        "        expected[row[-1]] = 1\n",
        "        sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
        "        backward_propagate_error(network, expected)\n",
        "        update_weights(network, row, l_rate)\n",
        "      print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPcL4JnDmvYK"
      },
      "source": [
        "# Initialize a network\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "\tnetwork = list()\n",
        "\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
        "\tnetwork.append(hidden_layer)\n",
        "\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
        "\tnetwork.append(output_layer)\n",
        "\treturn network"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRNkEsu5mxgQ"
      },
      "source": [
        "# Make a prediction with a network\n",
        "def predict(network, row):\n",
        "\toutputs = forward_propagate(network, row)\n",
        "\treturn outputs.index(max(outputs))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVheDKTMm-qC"
      },
      "source": [
        "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
        "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
        "\tn_inputs = len(train[0]) - 1\n",
        "\tn_outputs = len(set([row[-1] for row in train]))\n",
        "\tnetwork = initialize_network(n_inputs, n_hidden, n_outputs)\n",
        "\ttrain_network(network, train, l_rate, n_epoch, n_outputs)\n",
        "\tpredictions = list()\n",
        "\tfor row in test:\n",
        "\t\tprediction = predict(network, row)\n",
        "\t\tpredictions.append(prediction)\n",
        "\treturn(predictions)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_W8Wv7Ym_WC",
        "outputId": "62cc7b0b-bc60-4828-b76e-511fe32c8113"
      },
      "source": [
        "# load and prepare data\n",
        "filename = '/content/Machine-learning-with-Python/ML/Diabetes/diabetes.csv'\n",
        "dataset = load_csv(filename)\n",
        "for i in range(len(dataset[0])-1):\n",
        "\tstr_column_to_float(dataset, i)\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "# normalize input variables\n",
        "minmax = dataset_minmax(dataset)\n",
        "normalize_dataset(dataset, minmax)\n",
        "# evaluate algorithm\n",
        "n_folds = 2\n",
        "l_rate = 0.3\n",
        "n_epoch = 200\n",
        "n_hidden = 15\n",
        "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">epoch=0, lrate=0.300, error=383.812\n",
            ">epoch=1, lrate=0.300, error=383.766\n",
            ">epoch=2, lrate=0.300, error=383.684\n",
            ">epoch=3, lrate=0.300, error=383.483\n",
            ">epoch=4, lrate=0.300, error=379.464\n",
            ">epoch=5, lrate=0.300, error=343.059\n",
            ">epoch=6, lrate=0.300, error=342.141\n",
            ">epoch=7, lrate=0.300, error=341.052\n",
            ">epoch=8, lrate=0.300, error=339.542\n",
            ">epoch=9, lrate=0.300, error=287.988\n",
            ">epoch=10, lrate=0.300, error=179.216\n",
            ">epoch=11, lrate=0.300, error=174.055\n",
            ">epoch=12, lrate=0.300, error=167.667\n",
            ">epoch=13, lrate=0.300, error=160.774\n",
            ">epoch=14, lrate=0.300, error=154.368\n",
            ">epoch=15, lrate=0.300, error=149.044\n",
            ">epoch=16, lrate=0.300, error=144.825\n",
            ">epoch=17, lrate=0.300, error=141.493\n",
            ">epoch=18, lrate=0.300, error=138.834\n",
            ">epoch=19, lrate=0.300, error=136.688\n",
            ">epoch=20, lrate=0.300, error=134.934\n",
            ">epoch=21, lrate=0.300, error=133.479\n",
            ">epoch=22, lrate=0.300, error=132.257\n",
            ">epoch=23, lrate=0.300, error=131.219\n",
            ">epoch=24, lrate=0.300, error=130.329\n",
            ">epoch=25, lrate=0.300, error=129.558\n",
            ">epoch=26, lrate=0.300, error=128.885\n",
            ">epoch=27, lrate=0.300, error=128.292\n",
            ">epoch=28, lrate=0.300, error=127.764\n",
            ">epoch=29, lrate=0.300, error=127.291\n",
            ">epoch=30, lrate=0.300, error=126.861\n",
            ">epoch=31, lrate=0.300, error=126.469\n",
            ">epoch=32, lrate=0.300, error=126.107\n",
            ">epoch=33, lrate=0.300, error=125.771\n",
            ">epoch=34, lrate=0.300, error=125.457\n",
            ">epoch=35, lrate=0.300, error=125.162\n",
            ">epoch=36, lrate=0.300, error=124.883\n",
            ">epoch=37, lrate=0.300, error=124.619\n",
            ">epoch=38, lrate=0.300, error=124.367\n",
            ">epoch=39, lrate=0.300, error=124.126\n",
            ">epoch=40, lrate=0.300, error=123.895\n",
            ">epoch=41, lrate=0.300, error=123.673\n",
            ">epoch=42, lrate=0.300, error=123.459\n",
            ">epoch=43, lrate=0.300, error=123.252\n",
            ">epoch=44, lrate=0.300, error=123.052\n",
            ">epoch=45, lrate=0.300, error=122.858\n",
            ">epoch=46, lrate=0.300, error=122.669\n",
            ">epoch=47, lrate=0.300, error=122.486\n",
            ">epoch=48, lrate=0.300, error=122.307\n",
            ">epoch=49, lrate=0.300, error=122.133\n",
            ">epoch=50, lrate=0.300, error=121.963\n",
            ">epoch=51, lrate=0.300, error=121.797\n",
            ">epoch=52, lrate=0.300, error=121.634\n",
            ">epoch=53, lrate=0.300, error=121.474\n",
            ">epoch=54, lrate=0.300, error=121.317\n",
            ">epoch=55, lrate=0.300, error=121.164\n",
            ">epoch=56, lrate=0.300, error=121.013\n",
            ">epoch=57, lrate=0.300, error=120.864\n",
            ">epoch=58, lrate=0.300, error=120.718\n",
            ">epoch=59, lrate=0.300, error=120.574\n",
            ">epoch=60, lrate=0.300, error=120.432\n",
            ">epoch=61, lrate=0.300, error=120.291\n",
            ">epoch=62, lrate=0.300, error=120.153\n",
            ">epoch=63, lrate=0.300, error=120.017\n",
            ">epoch=64, lrate=0.300, error=119.883\n",
            ">epoch=65, lrate=0.300, error=119.750\n",
            ">epoch=66, lrate=0.300, error=119.619\n",
            ">epoch=67, lrate=0.300, error=119.490\n",
            ">epoch=68, lrate=0.300, error=119.363\n",
            ">epoch=69, lrate=0.300, error=119.237\n",
            ">epoch=70, lrate=0.300, error=119.113\n",
            ">epoch=71, lrate=0.300, error=118.991\n",
            ">epoch=72, lrate=0.300, error=118.870\n",
            ">epoch=73, lrate=0.300, error=118.751\n",
            ">epoch=74, lrate=0.300, error=118.634\n",
            ">epoch=75, lrate=0.300, error=118.519\n",
            ">epoch=76, lrate=0.300, error=118.405\n",
            ">epoch=77, lrate=0.300, error=118.293\n",
            ">epoch=78, lrate=0.300, error=118.182\n",
            ">epoch=79, lrate=0.300, error=118.073\n",
            ">epoch=80, lrate=0.300, error=117.966\n",
            ">epoch=81, lrate=0.300, error=117.861\n",
            ">epoch=82, lrate=0.300, error=117.757\n",
            ">epoch=83, lrate=0.300, error=117.655\n",
            ">epoch=84, lrate=0.300, error=117.554\n",
            ">epoch=85, lrate=0.300, error=117.455\n",
            ">epoch=86, lrate=0.300, error=117.358\n",
            ">epoch=87, lrate=0.300, error=117.262\n",
            ">epoch=88, lrate=0.300, error=117.167\n",
            ">epoch=89, lrate=0.300, error=117.075\n",
            ">epoch=90, lrate=0.300, error=116.983\n",
            ">epoch=91, lrate=0.300, error=116.893\n",
            ">epoch=92, lrate=0.300, error=116.804\n",
            ">epoch=93, lrate=0.300, error=116.717\n",
            ">epoch=94, lrate=0.300, error=116.631\n",
            ">epoch=95, lrate=0.300, error=116.547\n",
            ">epoch=96, lrate=0.300, error=116.463\n",
            ">epoch=97, lrate=0.300, error=116.381\n",
            ">epoch=98, lrate=0.300, error=116.300\n",
            ">epoch=99, lrate=0.300, error=116.221\n",
            ">epoch=100, lrate=0.300, error=116.142\n",
            ">epoch=101, lrate=0.300, error=116.065\n",
            ">epoch=102, lrate=0.300, error=115.988\n",
            ">epoch=103, lrate=0.300, error=115.913\n",
            ">epoch=104, lrate=0.300, error=115.839\n",
            ">epoch=105, lrate=0.300, error=115.766\n",
            ">epoch=106, lrate=0.300, error=115.693\n",
            ">epoch=107, lrate=0.300, error=115.622\n",
            ">epoch=108, lrate=0.300, error=115.552\n",
            ">epoch=109, lrate=0.300, error=115.482\n",
            ">epoch=110, lrate=0.300, error=115.413\n",
            ">epoch=111, lrate=0.300, error=115.345\n",
            ">epoch=112, lrate=0.300, error=115.278\n",
            ">epoch=113, lrate=0.300, error=115.211\n",
            ">epoch=114, lrate=0.300, error=115.145\n",
            ">epoch=115, lrate=0.300, error=115.080\n",
            ">epoch=116, lrate=0.300, error=115.016\n",
            ">epoch=117, lrate=0.300, error=114.952\n",
            ">epoch=118, lrate=0.300, error=114.889\n",
            ">epoch=119, lrate=0.300, error=114.826\n",
            ">epoch=120, lrate=0.300, error=114.764\n",
            ">epoch=121, lrate=0.300, error=114.702\n",
            ">epoch=122, lrate=0.300, error=114.641\n",
            ">epoch=123, lrate=0.300, error=114.580\n",
            ">epoch=124, lrate=0.300, error=114.519\n",
            ">epoch=125, lrate=0.300, error=114.460\n",
            ">epoch=126, lrate=0.300, error=114.400\n",
            ">epoch=127, lrate=0.300, error=114.341\n",
            ">epoch=128, lrate=0.300, error=114.282\n",
            ">epoch=129, lrate=0.300, error=114.224\n",
            ">epoch=130, lrate=0.300, error=114.166\n",
            ">epoch=131, lrate=0.300, error=114.108\n",
            ">epoch=132, lrate=0.300, error=114.051\n",
            ">epoch=133, lrate=0.300, error=113.994\n",
            ">epoch=134, lrate=0.300, error=113.937\n",
            ">epoch=135, lrate=0.300, error=113.881\n",
            ">epoch=136, lrate=0.300, error=113.825\n",
            ">epoch=137, lrate=0.300, error=113.769\n",
            ">epoch=138, lrate=0.300, error=113.713\n",
            ">epoch=139, lrate=0.300, error=113.658\n",
            ">epoch=140, lrate=0.300, error=113.603\n",
            ">epoch=141, lrate=0.300, error=113.548\n",
            ">epoch=142, lrate=0.300, error=113.494\n",
            ">epoch=143, lrate=0.300, error=113.440\n",
            ">epoch=144, lrate=0.300, error=113.386\n",
            ">epoch=145, lrate=0.300, error=113.332\n",
            ">epoch=146, lrate=0.300, error=113.278\n",
            ">epoch=147, lrate=0.300, error=113.225\n",
            ">epoch=148, lrate=0.300, error=113.172\n",
            ">epoch=149, lrate=0.300, error=113.119\n",
            ">epoch=150, lrate=0.300, error=113.067\n",
            ">epoch=151, lrate=0.300, error=113.014\n",
            ">epoch=152, lrate=0.300, error=112.962\n",
            ">epoch=153, lrate=0.300, error=112.910\n",
            ">epoch=154, lrate=0.300, error=112.858\n",
            ">epoch=155, lrate=0.300, error=112.807\n",
            ">epoch=156, lrate=0.300, error=112.756\n",
            ">epoch=157, lrate=0.300, error=112.705\n",
            ">epoch=158, lrate=0.300, error=112.654\n",
            ">epoch=159, lrate=0.300, error=112.603\n",
            ">epoch=160, lrate=0.300, error=112.552\n",
            ">epoch=161, lrate=0.300, error=112.502\n",
            ">epoch=162, lrate=0.300, error=112.452\n",
            ">epoch=163, lrate=0.300, error=112.402\n",
            ">epoch=164, lrate=0.300, error=112.353\n",
            ">epoch=165, lrate=0.300, error=112.303\n",
            ">epoch=166, lrate=0.300, error=112.254\n",
            ">epoch=167, lrate=0.300, error=112.205\n",
            ">epoch=168, lrate=0.300, error=112.156\n",
            ">epoch=169, lrate=0.300, error=112.107\n",
            ">epoch=170, lrate=0.300, error=112.058\n",
            ">epoch=171, lrate=0.300, error=112.010\n",
            ">epoch=172, lrate=0.300, error=111.962\n",
            ">epoch=173, lrate=0.300, error=111.914\n",
            ">epoch=174, lrate=0.300, error=111.866\n",
            ">epoch=175, lrate=0.300, error=111.818\n",
            ">epoch=176, lrate=0.300, error=111.771\n",
            ">epoch=177, lrate=0.300, error=111.724\n",
            ">epoch=178, lrate=0.300, error=111.677\n",
            ">epoch=179, lrate=0.300, error=111.630\n",
            ">epoch=180, lrate=0.300, error=111.583\n",
            ">epoch=181, lrate=0.300, error=111.537\n",
            ">epoch=182, lrate=0.300, error=111.491\n",
            ">epoch=183, lrate=0.300, error=111.445\n",
            ">epoch=184, lrate=0.300, error=111.399\n",
            ">epoch=185, lrate=0.300, error=111.353\n",
            ">epoch=186, lrate=0.300, error=111.308\n",
            ">epoch=187, lrate=0.300, error=111.263\n",
            ">epoch=188, lrate=0.300, error=111.218\n",
            ">epoch=189, lrate=0.300, error=111.173\n",
            ">epoch=190, lrate=0.300, error=111.129\n",
            ">epoch=191, lrate=0.300, error=111.084\n",
            ">epoch=192, lrate=0.300, error=111.040\n",
            ">epoch=193, lrate=0.300, error=110.996\n",
            ">epoch=194, lrate=0.300, error=110.953\n",
            ">epoch=195, lrate=0.300, error=110.909\n",
            ">epoch=196, lrate=0.300, error=110.866\n",
            ">epoch=197, lrate=0.300, error=110.823\n",
            ">epoch=198, lrate=0.300, error=110.780\n",
            ">epoch=199, lrate=0.300, error=110.738\n",
            ">epoch=0, lrate=0.300, error=303.348\n",
            ">epoch=1, lrate=0.300, error=188.757\n",
            ">epoch=2, lrate=0.300, error=186.174\n",
            ">epoch=3, lrate=0.300, error=182.761\n",
            ">epoch=4, lrate=0.300, error=178.420\n",
            ">epoch=5, lrate=0.300, error=173.170\n",
            ">epoch=6, lrate=0.300, error=167.451\n",
            ">epoch=7, lrate=0.300, error=161.890\n",
            ">epoch=8, lrate=0.300, error=156.779\n",
            ">epoch=9, lrate=0.300, error=152.011\n",
            ">epoch=10, lrate=0.300, error=147.458\n",
            ">epoch=11, lrate=0.300, error=143.163\n",
            ">epoch=12, lrate=0.300, error=139.284\n",
            ">epoch=13, lrate=0.300, error=135.951\n",
            ">epoch=14, lrate=0.300, error=133.201\n",
            ">epoch=15, lrate=0.300, error=130.980\n",
            ">epoch=16, lrate=0.300, error=129.197\n",
            ">epoch=17, lrate=0.300, error=127.756\n",
            ">epoch=18, lrate=0.300, error=126.576\n",
            ">epoch=19, lrate=0.300, error=125.594\n",
            ">epoch=20, lrate=0.300, error=124.766\n",
            ">epoch=21, lrate=0.300, error=124.058\n",
            ">epoch=22, lrate=0.300, error=123.446\n",
            ">epoch=23, lrate=0.300, error=122.913\n",
            ">epoch=24, lrate=0.300, error=122.444\n",
            ">epoch=25, lrate=0.300, error=122.030\n",
            ">epoch=26, lrate=0.300, error=121.662\n",
            ">epoch=27, lrate=0.300, error=121.332\n",
            ">epoch=28, lrate=0.300, error=121.035\n",
            ">epoch=29, lrate=0.300, error=120.766\n",
            ">epoch=30, lrate=0.300, error=120.522\n",
            ">epoch=31, lrate=0.300, error=120.298\n",
            ">epoch=32, lrate=0.300, error=120.092\n",
            ">epoch=33, lrate=0.300, error=119.901\n",
            ">epoch=34, lrate=0.300, error=119.723\n",
            ">epoch=35, lrate=0.300, error=119.557\n",
            ">epoch=36, lrate=0.300, error=119.401\n",
            ">epoch=37, lrate=0.300, error=119.253\n",
            ">epoch=38, lrate=0.300, error=119.113\n",
            ">epoch=39, lrate=0.300, error=118.979\n",
            ">epoch=40, lrate=0.300, error=118.851\n",
            ">epoch=41, lrate=0.300, error=118.729\n",
            ">epoch=42, lrate=0.300, error=118.611\n",
            ">epoch=43, lrate=0.300, error=118.497\n",
            ">epoch=44, lrate=0.300, error=118.386\n",
            ">epoch=45, lrate=0.300, error=118.279\n",
            ">epoch=46, lrate=0.300, error=118.175\n",
            ">epoch=47, lrate=0.300, error=118.074\n",
            ">epoch=48, lrate=0.300, error=117.975\n",
            ">epoch=49, lrate=0.300, error=117.878\n",
            ">epoch=50, lrate=0.300, error=117.784\n",
            ">epoch=51, lrate=0.300, error=117.691\n",
            ">epoch=52, lrate=0.300, error=117.600\n",
            ">epoch=53, lrate=0.300, error=117.511\n",
            ">epoch=54, lrate=0.300, error=117.423\n",
            ">epoch=55, lrate=0.300, error=117.337\n",
            ">epoch=56, lrate=0.300, error=117.252\n",
            ">epoch=57, lrate=0.300, error=117.169\n",
            ">epoch=58, lrate=0.300, error=117.087\n",
            ">epoch=59, lrate=0.300, error=117.006\n",
            ">epoch=60, lrate=0.300, error=116.927\n",
            ">epoch=61, lrate=0.300, error=116.848\n",
            ">epoch=62, lrate=0.300, error=116.771\n",
            ">epoch=63, lrate=0.300, error=116.696\n",
            ">epoch=64, lrate=0.300, error=116.621\n",
            ">epoch=65, lrate=0.300, error=116.547\n",
            ">epoch=66, lrate=0.300, error=116.475\n",
            ">epoch=67, lrate=0.300, error=116.404\n",
            ">epoch=68, lrate=0.300, error=116.334\n",
            ">epoch=69, lrate=0.300, error=116.265\n",
            ">epoch=70, lrate=0.300, error=116.197\n",
            ">epoch=71, lrate=0.300, error=116.130\n",
            ">epoch=72, lrate=0.300, error=116.064\n",
            ">epoch=73, lrate=0.300, error=115.999\n",
            ">epoch=74, lrate=0.300, error=115.934\n",
            ">epoch=75, lrate=0.300, error=115.871\n",
            ">epoch=76, lrate=0.300, error=115.809\n",
            ">epoch=77, lrate=0.300, error=115.748\n",
            ">epoch=78, lrate=0.300, error=115.687\n",
            ">epoch=79, lrate=0.300, error=115.627\n",
            ">epoch=80, lrate=0.300, error=115.568\n",
            ">epoch=81, lrate=0.300, error=115.510\n",
            ">epoch=82, lrate=0.300, error=115.452\n",
            ">epoch=83, lrate=0.300, error=115.395\n",
            ">epoch=84, lrate=0.300, error=115.339\n",
            ">epoch=85, lrate=0.300, error=115.283\n",
            ">epoch=86, lrate=0.300, error=115.228\n",
            ">epoch=87, lrate=0.300, error=115.173\n",
            ">epoch=88, lrate=0.300, error=115.119\n",
            ">epoch=89, lrate=0.300, error=115.065\n",
            ">epoch=90, lrate=0.300, error=115.012\n",
            ">epoch=91, lrate=0.300, error=114.959\n",
            ">epoch=92, lrate=0.300, error=114.906\n",
            ">epoch=93, lrate=0.300, error=114.854\n",
            ">epoch=94, lrate=0.300, error=114.802\n",
            ">epoch=95, lrate=0.300, error=114.750\n",
            ">epoch=96, lrate=0.300, error=114.698\n",
            ">epoch=97, lrate=0.300, error=114.647\n",
            ">epoch=98, lrate=0.300, error=114.596\n",
            ">epoch=99, lrate=0.300, error=114.545\n",
            ">epoch=100, lrate=0.300, error=114.494\n",
            ">epoch=101, lrate=0.300, error=114.443\n",
            ">epoch=102, lrate=0.300, error=114.392\n",
            ">epoch=103, lrate=0.300, error=114.342\n",
            ">epoch=104, lrate=0.300, error=114.291\n",
            ">epoch=105, lrate=0.300, error=114.240\n",
            ">epoch=106, lrate=0.300, error=114.189\n",
            ">epoch=107, lrate=0.300, error=114.138\n",
            ">epoch=108, lrate=0.300, error=114.086\n",
            ">epoch=109, lrate=0.300, error=114.035\n",
            ">epoch=110, lrate=0.300, error=113.983\n",
            ">epoch=111, lrate=0.300, error=113.931\n",
            ">epoch=112, lrate=0.300, error=113.879\n",
            ">epoch=113, lrate=0.300, error=113.826\n",
            ">epoch=114, lrate=0.300, error=113.773\n",
            ">epoch=115, lrate=0.300, error=113.720\n",
            ">epoch=116, lrate=0.300, error=113.666\n",
            ">epoch=117, lrate=0.300, error=113.611\n",
            ">epoch=118, lrate=0.300, error=113.556\n",
            ">epoch=119, lrate=0.300, error=113.501\n",
            ">epoch=120, lrate=0.300, error=113.445\n",
            ">epoch=121, lrate=0.300, error=113.388\n",
            ">epoch=122, lrate=0.300, error=113.330\n",
            ">epoch=123, lrate=0.300, error=113.272\n",
            ">epoch=124, lrate=0.300, error=113.213\n",
            ">epoch=125, lrate=0.300, error=113.153\n",
            ">epoch=126, lrate=0.300, error=113.092\n",
            ">epoch=127, lrate=0.300, error=113.030\n",
            ">epoch=128, lrate=0.300, error=112.967\n",
            ">epoch=129, lrate=0.300, error=112.903\n",
            ">epoch=130, lrate=0.300, error=112.839\n",
            ">epoch=131, lrate=0.300, error=112.773\n",
            ">epoch=132, lrate=0.300, error=112.706\n",
            ">epoch=133, lrate=0.300, error=112.638\n",
            ">epoch=134, lrate=0.300, error=112.569\n",
            ">epoch=135, lrate=0.300, error=112.499\n",
            ">epoch=136, lrate=0.300, error=112.427\n",
            ">epoch=137, lrate=0.300, error=112.355\n",
            ">epoch=138, lrate=0.300, error=112.281\n",
            ">epoch=139, lrate=0.300, error=112.206\n",
            ">epoch=140, lrate=0.300, error=112.131\n",
            ">epoch=141, lrate=0.300, error=112.054\n",
            ">epoch=142, lrate=0.300, error=111.976\n",
            ">epoch=143, lrate=0.300, error=111.897\n",
            ">epoch=144, lrate=0.300, error=111.818\n",
            ">epoch=145, lrate=0.300, error=111.738\n",
            ">epoch=146, lrate=0.300, error=111.657\n",
            ">epoch=147, lrate=0.300, error=111.575\n",
            ">epoch=148, lrate=0.300, error=111.493\n",
            ">epoch=149, lrate=0.300, error=111.410\n",
            ">epoch=150, lrate=0.300, error=111.328\n",
            ">epoch=151, lrate=0.300, error=111.245\n",
            ">epoch=152, lrate=0.300, error=111.162\n",
            ">epoch=153, lrate=0.300, error=111.080\n",
            ">epoch=154, lrate=0.300, error=110.997\n",
            ">epoch=155, lrate=0.300, error=110.915\n",
            ">epoch=156, lrate=0.300, error=110.833\n",
            ">epoch=157, lrate=0.300, error=110.752\n",
            ">epoch=158, lrate=0.300, error=110.672\n",
            ">epoch=159, lrate=0.300, error=110.592\n",
            ">epoch=160, lrate=0.300, error=110.513\n",
            ">epoch=161, lrate=0.300, error=110.435\n",
            ">epoch=162, lrate=0.300, error=110.358\n",
            ">epoch=163, lrate=0.300, error=110.282\n",
            ">epoch=164, lrate=0.300, error=110.207\n",
            ">epoch=165, lrate=0.300, error=110.133\n",
            ">epoch=166, lrate=0.300, error=110.060\n",
            ">epoch=167, lrate=0.300, error=109.987\n",
            ">epoch=168, lrate=0.300, error=109.916\n",
            ">epoch=169, lrate=0.300, error=109.846\n",
            ">epoch=170, lrate=0.300, error=109.777\n",
            ">epoch=171, lrate=0.300, error=109.709\n",
            ">epoch=172, lrate=0.300, error=109.641\n",
            ">epoch=173, lrate=0.300, error=109.575\n",
            ">epoch=174, lrate=0.300, error=109.510\n",
            ">epoch=175, lrate=0.300, error=109.445\n",
            ">epoch=176, lrate=0.300, error=109.381\n",
            ">epoch=177, lrate=0.300, error=109.318\n",
            ">epoch=178, lrate=0.300, error=109.256\n",
            ">epoch=179, lrate=0.300, error=109.194\n",
            ">epoch=180, lrate=0.300, error=109.133\n",
            ">epoch=181, lrate=0.300, error=109.073\n",
            ">epoch=182, lrate=0.300, error=109.013\n",
            ">epoch=183, lrate=0.300, error=108.954\n",
            ">epoch=184, lrate=0.300, error=108.895\n",
            ">epoch=185, lrate=0.300, error=108.837\n",
            ">epoch=186, lrate=0.300, error=108.779\n",
            ">epoch=187, lrate=0.300, error=108.722\n",
            ">epoch=188, lrate=0.300, error=108.665\n",
            ">epoch=189, lrate=0.300, error=108.608\n",
            ">epoch=190, lrate=0.300, error=108.552\n",
            ">epoch=191, lrate=0.300, error=108.496\n",
            ">epoch=192, lrate=0.300, error=108.440\n",
            ">epoch=193, lrate=0.300, error=108.385\n",
            ">epoch=194, lrate=0.300, error=108.330\n",
            ">epoch=195, lrate=0.300, error=108.274\n",
            ">epoch=196, lrate=0.300, error=108.219\n",
            ">epoch=197, lrate=0.300, error=108.165\n",
            ">epoch=198, lrate=0.300, error=108.110\n",
            ">epoch=199, lrate=0.300, error=108.055\n",
            "Scores: [74.73958333333334, 75.0]\n",
            "Mean Accuracy: 74.870%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MpvasAZj1rL"
      },
      "source": [
        ""
      ],
      "execution_count": 73,
      "outputs": []
    }
  ]
}